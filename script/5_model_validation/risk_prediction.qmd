---
title: "Contamination Risk Prediction"
format: html
toc: true
editor: visual
code-fold: true
warning: false 
message: false
self-contained: true
tabset: true
---

```{r, warning=FALSE,echo=FALSE,message=FALSE,results='hide'}
# load packages
library("MASS")
library("dplyr")
library("factoextra")
library("exactextractr")
library("tigris")
library("ggplot2")
library("plotly")
library("sf")
library("tibble")
library("GGally")
library("cluster")
library("mclust")
library("randomForest")
library("caret")
library('pROC')
library('smotefamily')
library("gstat")
library('sp')
library("pdp")

## read data
data <- st_read('nc_grid_validation.gpkg')
data$time <- as.numeric(as.Date(data$Date_Collected) - as.Date("2024-09-25"))
data <- data[, -c(1:15, 113)]
data <- st_transform(data, 4326)

# extract coordinates (for POINT geometries)
coords <- st_coordinates(data)

# add lon/lat as new columns
data$lon <- coords[,1]
data$lat <- coords[,2]

ref <- c(11:12, 21:24, 31, 41:43, 52, 71, 81, 82, 90, 95)

data$landcover <- as.numeric(sapply(data$landcover, function(val) {
  ref[which.min(abs(ref - val))]
}))
data$landcover <- as.factor(data$landcover)

## data for analysis
data_analysis <- st_drop_geometry(data)
data_analysis <- data_analysis[, -c(1:3)]
data_analysis <- na.omit(data_analysis)
data_analysis$Coliform_Result_Binary <- as.factor(data_analysis$Coliform_Result_Binary)

```

## Introduction

We recently obtained well testing results for private wells in the western counties of North Carolina, with a focus on total coliform contamination. The presence of total coliform in drinking water indicates potential structural vulnerabilities in the water system, such as cracks or leaks, that may allow harmful microbes to enter. The dataset includes approximately 800 private wells.

In this report, we apply a random forest model to predict contamination risk based on three sets of predictors: (1) spatial location, (2) time since the hurricane Helene affecting North Carolina, and (3) roughly 100 additional variables derived from capacity, vulnerability, and hazard modules. The data will be divided into training and testing sets using a 4:1 ratio.

Note that the dataset is imbalanced.

```{r}
table(data_analysis$Coliform_Result_Binary)
```

In view of this imbalance, we will maximize the F1 score to achieve a balance between specificity and sensitivity. In specific, it is defined as

$$
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

where the precision and recall are defined as

$$
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}, 
\quad 
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
$$

The F1 score balances precision and recall by their harmonic mean. Precision measures how many predicted contaminated wells are truly contaminated, and recall measures how many actual contaminated wells are correctly identified. Maximizing the F1 score helps select a cutoff that captures most contaminated wells while limiting false alarms.

According to the results, random forest achieves a **moderate** prediction performance (an AUC of 0.66). And, we do not detect strong spatial dependence in the residuals.

## Random forest

To fit the random forest, we first try to tune $\texttt{mtry}$ and $\texttt{nodesize}$. Note that

1.  \\(\\texttt{mtry}\\) stands for the number of variables randomly sampled as candidates at each split. If $\texttt{mtry}$ is small, it reduces variance of the forest by reducing correlation between trees but higher bias for each tree. If $\texttt{mtry}$ is large, it reduces bias of the forest but trees are more correlated. As a result, there is less efficiency gain and overfitting may occur.
2.  \\(\\texttt{nodesize}\\) specifies the minimum number of observations required in a terminal node (leaf) of a tree. Essentially, it controls how deep the trees can grow. If \\(\\texttt{nodesize}\\) is small, we have very deep trees, which have smaller bias but higher risk of overfitting. If \\(\\texttt{nodesize}\\) is large, we have shallower trees with higher bias and smaller variance.

The tuning is based on out-of-bag (OOB) error. OOB error is an internal estimate of prediction error. Each tree is trained on a bootstrap sample, leaving out some observations. Those are the OOB samples. After the tree is built, it predicts the OOB samples, and aggregating these predictions across all trees gives an error estimate for the full dataset. The current tuning suggests using \\(\\texttt{mtry}\\) of 7 and \\(\\texttt{nodesize}\\) of 3.

```{r}

# set a random seed for reproducibility
set.seed(666)

# training and testing split
train_index <- createDataPartition(data_analysis$Coliform_Result_Binary, p = 0.8, list = FALSE)
train_data <- data_analysis[train_index, ]
test_data  <- data_analysis[-train_index, ]

# Define search grids
mtry_grid <- c(3, 5, 7, 9, 11, 13)          
nodesize_grid <- 1:5

# Store results
results <- data.frame(
  mtry = integer(),
  nodesize = integer(),
  OOB_Error = numeric()
)

# Grid search
for (m in mtry_grid) {
  for (n in nodesize_grid) {
    set.seed(666)  
    rf_model <- randomForest(
      Coliform_Result_Binary ~ ., 
      data = train_data,
      ntree = 2000,        
      mtry = m,
      nodesize = n,
      importance = TRUE
    )
    
    # Get OOB error (classification)
    oob_error <- rf_model$err.rate[rf_model$ntree, "OOB"]
    
    # Save
    results <- rbind(results, data.frame(
      mtry = m,
      nodesize = n,
      OOB_Error = oob_error
    ))
  }
}

# Show results sorted by error
opt_res <- results[order(results$OOB_Error), ]
best_mtry <- opt_res[1, 1]
best_nodesize <- opt_res[1, 2]
print(c(best_mtry, best_nodesize))
```

### Variable importance

Variable importance measures how much each variable contributes to the random forest’s performance. Mean Decrease Accuracy indicates how much the model’s accuracy drops if a variable is randomly shuffled, while Mean Decrease Gini reflects how much a variable helps separate classes by reducing node impurity (how mixed the classes are within the node). High values in either metric indicate variables with high importance.

To improve the prediction results, we filter out variables with variable importance smaller than the 75th quantile of the variable importance across all the variables. This can help improve overfitting issues. The resulting dataset consists of 25 variables. Based on the updated set of variables, we further tune \\(\\texttt{mtry}\\) and \\(\\texttt{nodesize}\\). The current tuning suggests using \\(\\texttt{mtry}\\) of 13 and \\(\\texttt{nodesize}\\) of 5.

To interpret the results, we present the variable importance plot below.

```{r, message=FALSE}
# set a random seed for reproducibility
set.seed(666)
mod.rf <- randomForest(Coliform_Result_Binary ~ ., 
                       data = train_data, 
                       ntree = 2000,          
                       mtry = best_mtry, 
                       nodesize = best_nodesize,
                       importance = TRUE)

# Variable importance plot
var_imp <- importance(mod.rf)

# filter out variable importance < median
index <- which(var_imp[, 3] > quantile(var_imp[, 3], .75))
index <- setdiff(index, c(95))
index <- union(index, c(96, 97, 98))

subset <- train_data[, index]
subset$Coliform_Result_Binary <- train_data$Coliform_Result_Binary


# Store results
results <- data.frame(
  mtry = integer(),
  nodesize = integer(),
  OOB_Error = numeric()
)

# Grid search
for (m in mtry_grid) {
  for (n in nodesize_grid) {
    set.seed(666)  
    rf_model <- randomForest(
      Coliform_Result_Binary ~ ., 
      data = subset,
      ntree = 2000,        
      mtry = m,
      nodesize = n,
      importance = TRUE
    )
    
    # Get OOB error (classification)
    oob_error <- rf_model$err.rate[rf_model$ntree, "OOB"]
    
    # Save
    results <- rbind(results, data.frame(
      mtry = m,
      nodesize = n,
      OOB_Error = oob_error
    ))
  }
}

# Show results sorted by error
opt_res <- results[order(results$OOB_Error), ]
best_mtry <- opt_res[1, 1]
best_nodesize <- opt_res[1, 2]
print(c(best_mtry, best_nodesize))

set.seed(666)
mod.rf <- randomForest(Coliform_Result_Binary ~ ., 
                       data = subset, 
                       ntree = 2000,          
                       mtry = best_mtry, 
                       nodesize = best_nodesize,
                       importance = TRUE)

# Variable importance plot
var_imp <- importance(mod.rf)

# Top 10 by MeanDecreaseAccuracy
top10_acc <- data.frame(
  Variable = rownames(var_imp),
  Importance = var_imp[, "MeanDecreaseAccuracy"]
) %>%
  arrange(desc(Importance)) %>%
  slice(1:10)

ggplot(top10_acc, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "magenta") +
  coord_flip() +
  labs(title = "Top 10 Variables by Mean Decrease Accuracy",
       x = "", y = "Mean Decrease Accuracy") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.y = element_text(face = "bold"),
    panel.grid = element_blank()   # remove grid lines
  )

# Top 10 by MeanDecreaseGini
top10_gini <- data.frame(
  Variable = rownames(var_imp),
  Importance = var_imp[, "MeanDecreaseGini"]
) %>%
  arrange(desc(Importance)) %>%
  slice(1:10)

ggplot(top10_gini, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "deepskyblue") +
  coord_flip() +
  labs(title = "Top 10 Variables by Mean Decrease Gini",
       x = "", y = "Mean Decrease Gini") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.y = element_text(face = "bold"),
    panel.grid = element_blank()   
  )
```

In terms of model accuracy, the high contributing variables are (1) time to when the hurricane hits NC state, (2) precipitation, (3) Potassium Feldspar, (4) pH, and (5) temperature.

In terms of discriminatory power, the high contributing variables are (1) time to when the hurricane hits NC state, (2) pH, (3) percentage of new home, (4) agricultural runoff, and (5) groundwater Nitrate (domestic wells).

### Partial dependence plot

Next, we use partial dependence plot to visualize the effect of the variables on the predicted contamination risk. For a given variable, its partial dependence plot shows how the model’s predicted probability changes as that variable is varied, while averaging over the values of all other variables in the data. However, it is important to emphasize that **partial dependence plot does not account for confounding effects**. So, caution is needed when interpreting the results.

```{r}
vars <- c('time', 'ppt', 'A_Tot_K_fs',
          'pH', 'tmean', 
          'no3_dom', 'ag_runoff', 'pct_new_home')
xlabs <- c('Time', 'Precipitation', 'Potassium Feldspar',
           'pH', 'Temperature', 
           'Groundwater Nitrate (Domestic)', 'Agricultural Runoff', 
           'Percentage of New Home')
cols <- c('deepskyblue', 'magenta', 'indianred1', 
          'slateblue', 'royalblue', 'cyan2', 
          'springgreen', 'khaki2')

for (i in 1:8) {
  
  pdp_var <- partial(mod.rf, 
                     pred.var = vars[i],        
                     prob = TRUE,              
                     which.class = 2,
                     grid.resolution = 50,     
                     train = subset)  
  
  pdp_df <- as.data.frame(pdp_var)
  
  plot(x = pdp_df[, 1], y = pdp_df[, 2], type = 'l',
       col = cols[i], lwd = 2,
       main = paste0('Contamination Risk on ', xlabs[i]),
       xlab = xlabs[i], ylab = 'Predicted Probability')
}
```

Note that

1.  For time, the contamination risk first sharply decreases and then flattens.
2.  For precipitation, its relation with contamination risk turns out to be U-shape. When there is moderate rainfall, there’s enough water to flush contaminants downward and dilute them, but not too much to cause deep leaching.
3.  For potassium feldspar, its relation to contamination risk turns out to be U-shaped. If there is low feldspar content in the soils, it indicates soils are highly weathered, which can make the contaminants more mobile. When there is high feldspar content, soils are less weathered and may have lower capacity to hold water. So, contaminants may move more directly into groundwater.
4.  There is an U-shape relation between PH and contamination risk. Note that the minimum is reached around when PH is between 5.0 and 5.2. It is possible that the soil tends to trap contaminants better when PH is in this range. But the contaminants are more mobile and soluble in other ranges.
5.  For temperature (square root), its effect on contamination risk is flat when it is smaller than 3.8. However, it increases the contamination risk when it is greater than 3.8. It is expected as higher temperature implies more active microbiological and chemical activities.
6.  There is an U-shape relation between contamination risk and groundwater Nitrate (domestic). Note that a log transformation was applied to groundwater Nitrate. Moderate Nitrate levels may reflect a balanced recharge and dilution conditions. In other words, there is enough recharge to dilute Nitrate, but not so much infiltration that brings in many contaminants. When Nitrate levels are really high, it may indicate strong presence of agricultural or wastewater. When Nitrate levels are low, the contamination risk can still be present.
7.  For agricultural runoff, its effect on contamination risk is flat when agricultural runoff is between 20 and 48. But contamination risk suddenly reduces around 50 and increases again after 65. This may appear counterintuitive. There may exist some confounding factors. For example, areas with intense agriculture may coincide with less dense urban development, which could be a stronger driver of contamination risk.
8.  For % of new home, its relation to contamination risk is U-shaped. Older neighborhoods often have aging infrastructure that can increase contamination risk. Areas with many new constructions might have higher contamination risk due to rapid development, temporary infrastructure, or construction-related activities.

### Prediction

Next, we try to determine the cutoff for the contamination risk and assess its prediction performance.

```{r}
# estimate risk
pred_probs <- predict(mod.rf, newdata = test_data, 
                      type = "prob")[,2]

# find the cutoff that maximizes F1score
roc_obj <- roc(response = test_data$Coliform_Result_Binary,
               predictor = pred_probs,
               levels = c("Absent", "Present")) 

thresholds <- seq(0, 1, by = 0.01)
f1_results <- data.frame(threshold = thresholds,
                         precision = NA,
                         recall = NA,
                         F1 = NA)

for(i in seq_along(thresholds)){
  thresh <- thresholds[i]
  pred_class <- factor(ifelse(pred_probs > thresh, "Present", "Absent"),
                       levels = c("Absent", "Present"))
  
  cm <- confusionMatrix(pred_class, as.factor(test_data$Coliform_Result_Binary), positive = "Present")
  
  precision <- cm$byClass["Pos Pred Value"]
  recall <- cm$byClass["Sensitivity"]
  f1 <- 2 * precision * recall / (precision + recall)
  
  f1_results$precision[i] <- precision
  f1_results$recall[i] <- recall
  f1_results$F1[i] <- f1
}

# Find threshold with max F1
best_f1 <- f1_results[which.max(f1_results$F1), ]
best_f1

pred_class <- ifelse(pred_probs >= 0.31, 'Present', 'Absent')

# Confusion matrix
confusionMatrix(as.factor(pred_class), 
                as.factor(test_data$Coliform_Result_Binary),
                positive = 'Present')
# area under curve
auc(roc_obj)
```

The cutoff that maximizes F1 score is 0.31. The resulting model has an area under curve (AUC) of 0.66, which is better than random guess. Overall, the model is moderately effective at predicting well contamination. It correctly identifies a fair number of contaminated wells, but it also misses some, meaning some unsafe wells could go undetected. At the same time, it classifies a number of safe wells as contaminated, which could lead to unnecessary testing or concern.

## Test for spatial dependence

To diagnose the model performance, we check the spatial dependence of the residuals. We check the sample variogram of the residuals of the random forest model.

```{r}
# Get predicted probabilities
probs <- predict(mod.rf, train_data, type = "prob")

# residuals
y <- train_data$Coliform_Result_Binary
resid <- c()
for (i in 1:nrow(probs)) {
  if (y[i] == 'Present') {
    resid[i] <- 1 - probs[i, 2]
  } else {
    resid[i] <- probs[i, 1]
  }
}
train_data$resid <- resid

# Compute empirical variogram of residuals
coordinates(train_data) <- ~ lon + lat   # makes it Spatial
vgm_resid <- variogram(resid ~ 1, train_data)
```

We assume a exponential variogram model to describe the spatial dependence, detailed as follows.

$$
\gamma(h) = c_0 + c \left( 1 - \exp\left(-\frac{h}{a}\right) \right)
$$

Note that

-   \\(\\gamma(h)\\) stands for semivariogram, which measures how similar residuals are as a function of distance \\(h\\).

-   $a$ stands for the range (scale of spatial dependence), which controls how quickly the spatial correlation decays.

-   \\(c_0\\) stands for the nugget effect, which represents measurement error.

-   \\(c\\) stands for partial sill, which is the part of variance (\\(c_0 + c\\)) that is explained by spatial correlation.

The variogram fitted by exponential model is shown below. Note that the points are the sample variogram while the line represents the fitted variogram. Note that the variogram appears to fluctuate around a constant level. It suggests that the spatial dependence in the residuals may not be strong.

```{r}
vgm.mod <- fit.variogram(vgm_resid, model = vgm("Exp"))
print(vgm.mod)
plot(vgm_resid, vgm.mod, lty = 2, type = 'b', col = 'magenta',
     ylab = 'Semivariance', xlab = 'Distance',
     main = 'Variogram (exponential model)')
```

According to the estimation results, we see the partial sill is 0.016 and the nugget effect is 0.080. Only \\(0.018 / (0.061 + 0.018) = 23\\%\\) of the residual variance is due to spatial dependence. So, some spatial dependence exists in the residuals but they are relatively weak compared to the noise. Next, we conduct a formal statistical test for the spatial dependence using permutation test.

```{r}
set.seed(666)
nperm <- 1000
perm.psills <- numeric(nperm)
obs.psill <- vgm.mod$psill[2]
for (i in 1:nperm) {
  # Permute residuals
  train_data$resid.perm <- sample(train_data$resid)
  
  # Empirical variogram
  vg.perm <- variogram(resid.perm ~ 1, train_data)
  
  # Fit spherical model
  fit.perm <- fit.variogram(vg.perm, model = vgm("Exp"))
  
  # Record partial sill
  perm.psills[i] <- fit.perm$psill[2]
}
(p <- mean(perm.psills >= obs.psill))
```

According to the results above, we can see that the partial sill is not significant. So, it suggests that the spatial dependence is weak.
